{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\coding\\anaconda\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\coding\\anaconda\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Requirement already satisfied: numpy in c:\\coding\\anaconda\\lib\\site-packages (1.18.1)\n",
      "Requirement already satisfied: pandas in c:\\coding\\anaconda\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\coding\\anaconda\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\coding\\anaconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\coding\\anaconda\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\coding\\anaconda\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\coding\\anaconda\\lib\\site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\coding\\anaconda\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in c:\\coding\\anaconda\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\coding\\anaconda\\lib\\site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\coding\\anaconda\\lib\\site-packages (from h5py->keras) (1.14.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Haoran\n",
      "[nltk_data]     Li\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Haoran\n",
      "[nltk_data]     Li\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download ('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides a lot of packages to load files in different formats. We provide a simple data loader to help you load .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"id\", \"label\"]\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **feature extractor** is one of the most important parts in a pipeline.\n",
    "In this tutorial, we introduce four different functions to extract features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sentence. *'Text mining is to identify useful information.'*, this function is used to tokenize it to a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(\"Text mining is to identify useful information.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next part is stemming. Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = stem(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single word is sometimes weakly expressive so that n-gram is a common method about better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n"
     ]
    }
   ],
   "source": [
    "bi_gram = n_gram(tokens, 2)\n",
    "print(bi_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language, some words are high-frequency but meaningless. We usually filter these words out to reduce the size of feature space. We can further filter features with low frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filter_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we get all features, the next step is to make these features suitable for the following models.\n",
    "One simple and common way is to use the one-hot vector (maybe it is not the best solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0.]\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "print(get_onehot_vector(tokens, {\"text\": 0, \"mine\": 1, \"COMP\": 2, \"HKUST\": 3}))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, \"text\" and \"mine\" appears in the tokens but \"COMP\" and \"HKUST\" not. So the 0 th, 1 th elements are ones and other elements are ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we introduce a 1-layer perceptron to classify reviews. This perceptron includes 1 dense layer with the softmax activation.\n",
    "Keras is the easiest deep learning framework so that we choose it to build this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "def build_classifier(input_size, output_size, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param learning_rate: the learning rate for SGD\n",
    "    return a 1-layer perceptron,\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add 1 layer with softmax activation\n",
    "    # you should specify the output size, the input_size, and the activation function\n",
    "    model.add(Dense(output_size, activation=\"softmax\", input_dim=input_size))\n",
    "    \n",
    "    # set the loss as categorical_crossentropy, the metric as accuracy, and the optimizer as SGD\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect All Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data loader, feature extractor, and the classifier. We can connect them to finish this pipeline of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 100870\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/ans.csv\"\n",
    "pred_file = \"data/pred.csv\"\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "# extract features\n",
    "\n",
    "# tokenization\n",
    "# the input is the text and the output is the word list\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "# stemming\n",
    "# the input is the word list and the ouput is the stemmed word list\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "# 2-gram\n",
    "# the input can be either the stemmed tokens or the original tokens\n",
    "# and the ouput is the 2_gram representation list\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "\n",
    "# remove stopwords\n",
    "# the input should be the stemmed tokens and the output is a cleanner token list\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "# build the feature list\n",
    "train_feats = list()\n",
    "for i in range(len(train_ids)):\n",
    "    # concatenate the stemmed token list and the 2_gram list together\n",
    "    train_feats.append(train_stemmed[i] + train_2_gram[i])\n",
    "test_feats = list()\n",
    "for i in range(len(test_ids)):\n",
    "    # concatenate the stemmed token list and the 2_gram list together\n",
    "    test_feats.append(test_stemmed[i] + test_2_gram[i])\n",
    "\n",
    "# build the feature dict\n",
    "feats = set()\n",
    "# collect all features\n",
    "for f in train_feats:\n",
    "    feats.update(f)\n",
    "print(\"Size of features:\", len(feats))\n",
    "# build a mapping from features to indices\n",
    "feats_dict = dict(zip(feats, range(len(feats))))\n",
    "\n",
    "# build the feats_matrix\n",
    "# convert each example to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in train_feats])\n",
    "test_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in test_feats])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(train_labels)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = K.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
    "test_label_matrix = K.utils.to_categorical(test_labels-1, num_classes=num_classes)\n",
    "\n",
    "# build the classifier\n",
    "model = build_classifier(len(feats_dict), num_classes, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the training hyperparameters to train a classifier. We set the total_epoch as 10, the batch_size as 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 494us/step - loss: 1.3930 - accuracy: 0.4435\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 1s 470us/step - loss: 1.1350 - accuracy: 0.5440\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 1s 454us/step - loss: 0.9896 - accuracy: 0.6245\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 1s 479us/step - loss: 0.8879 - accuracy: 0.6875\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 1s 460us/step - loss: 0.8085 - accuracy: 0.7420\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 1s 458us/step - loss: 0.7443 - accuracy: 0.7785\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 1s 452us/step - loss: 0.6921 - accuracy: 0.8055\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 1s 454us/step - loss: 0.6464 - accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 1s 444us/step - loss: 0.6069 - accuracy: 0.8490\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.5724 - accuracy: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21a3a418508>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "model.fit(train_feats_matrix, train_label_matrix, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this pipeline for test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 403us/step\n",
      "400/400 [==============================] - 0s 415us/step\n",
      "training loss: 0.5424470275640487 training accuracy 0.8790000081062317\n",
      "test loss: 1.0928072929382324 test accuracy 0.5649999976158142\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix, batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix, batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different hyperparameters by yourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our classifier here is overfitted. The training loss is much smaller than the test loss.\n",
    "Later we will introduce more strategies to make classifiers better generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "test_pred = model.predict_classes(test_feats_matrix)\n",
    "write_predictions(pred_file, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
