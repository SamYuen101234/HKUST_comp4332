{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Project 1 - Sentiment Classification\n",
    "\n",
    "Hello everyone, this is Zihao. I am very happy to host the first project\n",
    "\n",
    "In this project, you will conduct a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. stars, from 1-5) of each review.\n",
    "For each review, you are given a piece of text as well as some other features (Explore yourself!).\n",
    "You can consider the predicted variables to be categorical, ordinal or numerical.\n",
    "\n",
    "DDL: *April 6, 2021*\n",
    "- *March 23, 2021* release the validation score of weak baseline\n",
    "- *March 30, 2021* release the validation score of strong baseline\n",
    "\n",
    "Submission: Each team leader is required to submit the groupNo.zip file in the canvas. It shoud contain \n",
    "- `pre.csv` Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py)\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted.)\n",
    "\n",
    "We will check your report with your code and the accuracy.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 60%   | an easy baseline that most students can outperform                 | algorithm you used                |\n",
    "| 80%   | a competitive baseline that about half students can surpass        | detailed explanation              |\n",
    "| 90%   | a very competitive baseline without any special mechanism          | detailed explanation and analysis, such as explorative data analysis and ablation study |\n",
    "| 100%  | a very competitive baseline with at least one mechanism            | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you are provided with the code snippets for you to start.\n",
    "\n",
    "The content follows previous lectures and tutorials. But I may mention some useful python packages.\n",
    "\n",
    "## Instruction Content\n",
    "\n",
    "1. Load & Dump the data\n",
    "    1. Load the data\n",
    "    1. Dump the data\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines\n",
    "\n",
    "## 1. Load & Dump the data\n",
    "\n",
    "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
    "The key ingredient of our operation is the `DataFrame` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load the data\n",
    "\n",
    "Here is a function to load your data, remember put the dataset in the `data_2021_spring` folder.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars']):\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"succeed!\")\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Failed, then try to \")\n",
    "        print(f\"select all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'data_2021_spring/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text] columns from the train split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Better add a dollar sign to this profile! New ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great wings and great atmosphere to watch the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Must be Chinese to get good service... But its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great casual atmosphere - sorta retro trendy. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great for groups/games and delicious, lovingly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Better add a dollar sign to this profile! New ...\n",
       "1  Great wings and great atmosphere to watch the ...\n",
       "2  Must be Chinese to get good service... But its...\n",
       "3  Great casual atmosphere - sorta retro trendy. ...\n",
       "4  Great for groups/games and delicious, lovingly..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the test split\n",
      "Failed, then try to \n",
      "select all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "test_df = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dump the random answer\n",
    "\n",
    "In this project, your predictions on test data are supposed to be submitted by a csv file of two columns, i.e. (review_id and stars)\n",
    "\n",
    "Here we compose the random answer in a DataFrame and dump the answer into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ans = pd.DataFrame(data={\n",
    "    'review_id': test_df['review_id'],\n",
    "    'stars': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d616vNp4tZoaz-FV19bbjA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uZKXjPapFsHRA69QEVjRRg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l_uJ4NI6Nv2F4Nua6kOmpQ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eW6E9mFi6s1zI0ELbw9zHw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ty4rmFcO35uw5Ni7N7HQNQ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  d616vNp4tZoaz-FV19bbjA      0\n",
       "1  uZKXjPapFsHRA69QEVjRRg      2\n",
       "2  l_uJ4NI6Nv2F4Nua6kOmpQ      3\n",
       "3  eW6E9mFi6s1zI0ELbw9zHw      0\n",
       "4  ty4rmFcO35uw5Ni7N7HQNQ      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_number = -1\n",
    "random_ans.to_csv(f'{group_number}-random_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Preprocessing and feature engineering is important in machine learning\n",
    "\n",
    "### A. Text data processing recap\n",
    "In our tutorials, Haoran have showed you how to extract textual features by the `nltk` package\n",
    "\n",
    "Remember to use the NLTK Downloader to obtain the resource:\n",
    "```\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  >>> nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [food, okay, ,, servers, nice, need, experienc...\n",
      "1    [hung, bar, hours, watch, world, cup, match, c...\n",
      "2    [i, love, dr., randy, !, omg, ,, i, scared, go...\n",
      "3    [david, great, !, on, time, ,, knew, stuff, ,,...\n",
      "4    [go, candice, !, great, service, ., the, spina...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `nltk`, I would like to introduce `SpaCy`, a newer text processing toolkit of industrial strength.\n",
    "\n",
    "You can explore it at https://spacy.io/\n",
    "\n",
    "Let's install it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy enables you use linguistic features of texts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
      "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
      "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at, startup],\t \n",
      "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
      "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
      "U.K.      ,\t U.K.      ,\t PROPN     ,\t dobj      ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
      "startup   ,\t startup   ,\t NOUN      ,\t advcl     ,\t xxxx      ,\t True      ,\t False     ,\t [for]     ,\t \n",
      "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
      "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
      "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
      "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "fmt = \"{:10s},\\t \" * 8\n",
    "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
    "print('-'*140)\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows you use the embeddings for both sentence and words\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion [ 0.4847193   0.34561655  0.23650904 -0.27294627  0.30828613] ...\n",
      "Apple [ 0.9396687   0.46727175 -0.3862503  -0.23296848  0.25683203] ...\n",
      "is [-0.21470308 -0.36800703  1.8618155  -0.43874717 -0.6448474 ] ...\n",
      "looking [ 1.5960355  -0.01218066 -0.1948367   0.7979922   0.36900565] ...\n",
      "at [-1.2617028  -0.8116296  -0.55736023  0.08604071 -0.43663728] ...\n",
      "buying [ 0.3020423  -0.9611639   1.2695026   0.10633498  2.8583994 ] ...\n",
      "U.K. [ 2.2959712   0.78135234 -1.0174923  -0.5566485   0.69199914] ...\n",
      "startup [0.6782811  0.03798376 0.07798427 0.1210558  0.5636424 ] ...\n",
      "for [-0.07904667 -0.21996386 -1.3529027  -0.24131706  0.43687835] ...\n",
      "$ [ 0.44878927  0.75564337  0.5757578  -1.1713823   0.7438692 ] ...\n",
      "1 [-0.3846085  2.7049747  2.7081459 -1.4393395 -0.5412608] ...\n",
      "billion [ 1.011186    1.4275012  -0.38276425 -0.03342953 -0.9067332 ] ...\n"
     ]
    }
   ],
   "source": [
    "print(doc, doc.vector[:5], '...')\n",
    "for t in doc:\n",
    "    print(t, t.vector[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage of SpaCy, you can refer to the documentation of spacy https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Explorative data analysis\n",
    "\n",
    "For our dataset, we have features more than text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [f, u, l, l] columns from the train split\n",
      "Failed, then try to \n",
      "select all columns from the train split\n"
     ]
    }
   ],
   "source": [
    "train_df_full = load_data('train', columns='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VFRSCkkLM_h3qWpVXPd5VA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-06 05:18:21</td>\n",
       "      <td>0</td>\n",
       "      <td>ZJMnE7RfDVoMxgAxwNHh0w</td>\n",
       "      <td>2</td>\n",
       "      <td>Better add a dollar sign to this profile! New ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9gecGjcezpyzKbrHvYu1Eg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I5KYmFYPr9SEllG9L_qszA</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-24 16:14:48</td>\n",
       "      <td>0</td>\n",
       "      <td>FzZd9YnbP1UQzNx0JKu8_A</td>\n",
       "      <td>3</td>\n",
       "      <td>Great wings and great atmosphere to watch the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>UDK3hs9_Tc1lYUCZpjOkaQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fcWkZgmzRsm3H4egzSrV_A</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-25 20:11:30</td>\n",
       "      <td>0</td>\n",
       "      <td>NCEKd4_Vl_4NEvWtBSB0sQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Must be Chinese to get good service... But its...</td>\n",
       "      <td>0</td>\n",
       "      <td>AXpMT-1qgiTcNhSrCHR7bA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FUtlsQrJI7LhqDPxLumEw</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-12-16 07:55:16</td>\n",
       "      <td>1</td>\n",
       "      <td>le_3mT5UBgbTJDBXYPKsrw</td>\n",
       "      <td>3</td>\n",
       "      <td>Great casual atmosphere - sorta retro trendy. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>QYUYuDZEAeghBHz8ZXrhkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H5PdoEWleg9uDU67ZAy6JA</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-02-01 16:01:00</td>\n",
       "      <td>0</td>\n",
       "      <td>HZULRZCtoGX5FpEhYnCuJw</td>\n",
       "      <td>5</td>\n",
       "      <td>Great for groups/games and delicious, lovingly...</td>\n",
       "      <td>0</td>\n",
       "      <td>YE-gFdUrIvUz9WoO09PftQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  VFRSCkkLM_h3qWpVXPd5VA     0  2015-02-06 05:18:21      0   \n",
       "1  I5KYmFYPr9SEllG9L_qszA     0  2011-03-24 16:14:48      0   \n",
       "2  fcWkZgmzRsm3H4egzSrV_A     0  2014-09-25 20:11:30      0   \n",
       "3  0FUtlsQrJI7LhqDPxLumEw     0  2007-12-16 07:55:16      1   \n",
       "4  H5PdoEWleg9uDU67ZAy6JA     4  2017-02-01 16:01:00      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  ZJMnE7RfDVoMxgAxwNHh0w      2   \n",
       "1  FzZd9YnbP1UQzNx0JKu8_A      3   \n",
       "2  NCEKd4_Vl_4NEvWtBSB0sQ      4   \n",
       "3  le_3mT5UBgbTJDBXYPKsrw      3   \n",
       "4  HZULRZCtoGX5FpEhYnCuJw      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Better add a dollar sign to this profile! New ...       1   \n",
       "1  Great wings and great atmosphere to watch the ...       0   \n",
       "2  Must be Chinese to get good service... But its...       0   \n",
       "3  Great casual atmosphere - sorta retro trendy. ...       0   \n",
       "4  Great for groups/games and delicious, lovingly...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  9gecGjcezpyzKbrHvYu1Eg  \n",
       "1  UDK3hs9_Tc1lYUCZpjOkaQ  \n",
       "2  AXpMT-1qgiTcNhSrCHR7bA  \n",
       "3  QYUYuDZEAeghBHz8ZXrhkQ  \n",
       "4  YE-gFdUrIvUz9WoO09PftQ  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the relationship between different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fca92539400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train_df_full['cool'], train_df_full['funny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2977., 1566., 2204., 4426., 8827.]),\n",
       " array([1. , 1.8, 2.6, 3.4, 4.2, 5. ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df_full['stars'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you may use the id feature to aggregate data samples\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  business_id  funny  cool  stars\n",
      "9384   --9e1ONYQuAa-CB_Rrw7Tw      0     0      5\n",
      "11655  --9e1ONYQuAa-CB_Rrw7Tw      3     9      4\n",
      "14060  --9e1ONYQuAa-CB_Rrw7Tw      0     0      3\n",
      "15474  --9e1ONYQuAa-CB_Rrw7Tw      0     2      4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('business_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['business_id', 'funny', 'cool', 'stars']].head())\n",
    "        plt.hist(sub_df['stars'], bins=5)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      user_id  funny  cool  stars\n",
      "13385  -1ZMRA0N01rqZL0TWk3fgA      0     0      5\n",
      "13880  -1ZMRA0N01rqZL0TWk3fgA      0     0      4\n"
     ]
    }
   ],
   "source": [
    "for bid, sub_df in train_df_full.groupby('user_id'):\n",
    "    if len(sub_df) > 1:\n",
    "        print(sub_df[['user_id', 'funny', 'cool', 'stars']].head())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines\n",
    "\n",
    "Finally, we come up with two baselines for you to refer.\n",
    "We only use text data here and only consider first 5k training samples.\n",
    "\n",
    "For example, a baseline can be a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "succeed!\n",
      "select [text, stars] columns from the valid split\n",
      "succeed!\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train')[:5000]\n",
    "valid_df = load_data('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split above is what we have done for you. You can use the data as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fcaa5af05e0>)),\n",
      "                ('lr', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression()\n",
    "steps = [('tfidf', tfidf),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x7fcaa5af05e0>)),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.71      0.69       277\n",
      "           2       0.43      0.12      0.18       172\n",
      "           3       0.39      0.15      0.21       224\n",
      "           4       0.46      0.45      0.46       435\n",
      "           5       0.70      0.89      0.78       892\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.53      0.46      0.46      2000\n",
      "weighted avg       0.58      0.62      0.58      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[197  12   7  14  47]\n",
      " [ 57  20  25  35  35]\n",
      " [ 26  12  33  96  57]\n",
      " [  9   3  17 196 210]\n",
      " [  9   0   2  85 796]]\n",
      "accuracy 0.621\n"
     ]
    }
   ],
   "source": [
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "y_pred = pipe.predict(x_valid)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use deep learning.\n",
    "Here is a pytorch based baseline using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df['text'].map(tokenize).map(filter_stopwords).map(stem)\n",
    "valid_text = valid_df['text'].map(tokenize).map(filter_stopwords).map(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for tokens in train_text:\n",
    "    for t in tokens:\n",
    "        if not t in word2id:\n",
    "            word2id[t] = len(word2id)\n",
    "word2id['<pad>'] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_id_seq(texts, padding_length=500):\n",
    "    records = []\n",
    "    for tokens in texts:\n",
    "        record = []\n",
    "        for t in tokens:\n",
    "            record.append(word2id.get(t, len(word2id)))\n",
    "        if len(record) >= padding_length:\n",
    "            records.append(record[:padding_length])\n",
    "        else:\n",
    "            records.append(record + [word2id['<pad>']] * (padding_length - len(record)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = texts_to_id_seq(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = texts_to_id_seq(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seq, y):\n",
    "        assert len(seq) == len(y)\n",
    "        self.seq = seq\n",
    "        self.y = y-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.seq[idx]), self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(MyDataset(train_seqs, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(MyDataset(valid_seqs, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2id)+1, embedding_dim=64)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        x = torch.max(x, dim=-1)[0]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/313 [00:00<00:19, 15.80it/s, loss=0.121, acc=0.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:17<00:00, 17.60it/s, loss=0.0855, acc=0.44] \n",
      "100%|| 125/125 [00:01<00:00, 82.79it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.77it/s, loss=0.0756, acc=0.479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.32      0.37       277\n",
      "           1       0.00      0.00      0.00       172\n",
      "           2       0.00      0.00      0.00       224\n",
      "           3       0.29      0.65      0.40       435\n",
      "           4       0.69      0.64      0.66       892\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.28      0.32      0.29      2000\n",
      "weighted avg       0.43      0.47      0.43      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 90   0   0 121  66]\n",
      " [ 36   0   0 109  27]\n",
      " [ 22   0   0 173  29]\n",
      " [ 22   0   0 282 131]\n",
      " [ 42   0   0 282 568]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:17<00:00, 17.46it/s, loss=0.0718, acc=0.539]\n",
      "100%|| 125/125 [00:01<00:00, 77.12it/s]\n",
      "  1%|          | 2/313 [00:00<00:19, 16.31it/s, loss=0.0597, acc=0.604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.74      0.54       277\n",
      "           1       0.00      0.00      0.00       172\n",
      "           2       1.00      0.00      0.01       224\n",
      "           3       0.34      0.57      0.42       435\n",
      "           4       0.74      0.66      0.70       892\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.50      0.39      0.33      2000\n",
      "weighted avg       0.57      0.52      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[205   0   0  47  25]\n",
      " [ 78   0   0  72  22]\n",
      " [ 64   0   1 134  25]\n",
      " [ 54   0   0 247 134]\n",
      " [ 76   0   0 231 585]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:18<00:00, 17.25it/s, loss=0.0598, acc=0.617]\n",
      "100%|| 125/125 [00:01<00:00, 76.89it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.73it/s, loss=0.0443, acc=0.708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.64      0.58       277\n",
      "           1       0.33      0.01      0.01       172\n",
      "           2       0.33      0.13      0.19       224\n",
      "           3       0.35      0.57      0.43       435\n",
      "           4       0.72      0.70      0.71       892\n",
      "\n",
      "    accuracy                           0.54      2000\n",
      "   macro avg       0.45      0.41      0.39      2000\n",
      "weighted avg       0.54      0.54      0.51      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[178   1  16  42  40]\n",
      " [ 63   1  19  66  23]\n",
      " [ 33   1  30 129  31]\n",
      " [ 22   0  21 246 146]\n",
      " [ 38   0   6 222 626]]\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:18<00:00, 17.35it/s, loss=0.0486, acc=0.689]\n",
      "100%|| 125/125 [00:01<00:00, 81.57it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.03it/s, loss=0.0462, acc=0.604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.41      0.52       277\n",
      "           1       0.14      0.01      0.01       172\n",
      "           2       0.17      0.01      0.02       224\n",
      "           3       0.30      0.59      0.40       435\n",
      "           4       0.67      0.72      0.70       892\n",
      "\n",
      "    accuracy                           0.51      2000\n",
      "   macro avg       0.40      0.35      0.33      2000\n",
      "weighted avg       0.50      0.51      0.47      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[113   4   5  81  74]\n",
      " [ 24   1   9 107  31]\n",
      " [  9   1   3 169  42]\n",
      " [  3   0   1 258 173]\n",
      " [  5   1   0 240 646]]\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:17<00:00, 17.39it/s, loss=0.0375, acc=0.776]\n",
      "100%|| 125/125 [00:01<00:00, 74.69it/s]\n",
      "  1%|          | 2/313 [00:00<00:19, 16.06it/s, loss=0.0302, acc=0.854]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.58       277\n",
      "           1       0.33      0.17      0.23       172\n",
      "           2       0.25      0.46      0.33       224\n",
      "           3       0.35      0.34      0.35       435\n",
      "           4       0.73      0.69      0.71       892\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.46      0.44      0.44      2000\n",
      "weighted avg       0.55      0.52      0.53      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[152  24  51  14  36]\n",
      " [ 36  30  67  22  17]\n",
      " [ 25  14 103  54  28]\n",
      " [  9  12 119 148 147]\n",
      " [ 21  10  65 181 615]]\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:18<00:00, 17.23it/s, loss=0.0276, acc=0.841]\n",
      "100%|| 125/125 [00:01<00:00, 77.66it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.92it/s, loss=0.0241, acc=0.896]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.72      0.57       277\n",
      "           1       0.26      0.08      0.12       172\n",
      "           2       0.26      0.35      0.30       224\n",
      "           3       0.33      0.54      0.41       435\n",
      "           4       0.83      0.47      0.60       892\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.43      0.44      0.40      2000\n",
      "weighted avg       0.56      0.48      0.48      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[200  11  34  23   9]\n",
      " [ 78  14  47  30   3]\n",
      " [ 47  12  79  74  12]\n",
      " [ 29  11  96 237  62]\n",
      " [ 69   6  50 344 423]]\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:18<00:00, 17.24it/s, loss=0.0176, acc=0.917]\n",
      "100%|| 125/125 [00:01<00:00, 75.83it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.05it/s, loss=0.014, acc=0.958] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.56       277\n",
      "           1       0.27      0.22      0.24       172\n",
      "           2       0.30      0.29      0.30       224\n",
      "           3       0.32      0.54      0.40       435\n",
      "           4       0.77      0.54      0.64       892\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.44      0.43      0.43      2000\n",
      "weighted avg       0.55      0.49      0.50      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[157  38  27  35  20]\n",
      " [ 52  37  29  41  13]\n",
      " [ 27  27  66  90  14]\n",
      " [ 14  23  68 237  93]\n",
      " [ 31  14  30 335 482]]\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:17<00:00, 17.50it/s, loss=0.0113, acc=0.952] \n",
      "100%|| 125/125 [00:01<00:00, 77.09it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 17.01it/s, loss=0.00983, acc=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.69      0.58       277\n",
      "           1       0.21      0.05      0.08       172\n",
      "           2       0.37      0.18      0.24       224\n",
      "           3       0.35      0.56      0.43       435\n",
      "           4       0.74      0.64      0.68       892\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.43      0.42      0.40      2000\n",
      "weighted avg       0.53      0.53      0.51      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[190  12  14  35  26]\n",
      " [ 78   9  18  48  19]\n",
      " [ 47   6  40 102  29]\n",
      " [ 28   9  28 242 128]\n",
      " [ 39   6   8 270 569]]\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:17<00:00, 17.48it/s, loss=0.00644, acc=0.979]\n",
      "100%|| 125/125 [00:01<00:00, 81.43it/s]\n",
      "  1%|          | 2/313 [00:00<00:18, 16.76it/s, loss=0.00403, acc=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.79      0.55       277\n",
      "           1       0.21      0.15      0.18       172\n",
      "           2       0.33      0.07      0.11       224\n",
      "           3       0.35      0.49      0.41       435\n",
      "           4       0.75      0.60      0.67       892\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.41      0.42      0.38      2000\n",
      "weighted avg       0.53      0.50      0.49      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[220  17   3  21  16]\n",
      " [ 95  26  10  32   9]\n",
      " [ 64  35  15  86  24]\n",
      " [ 58  28  12 212 125]\n",
      " [ 86  15   5 254 532]]\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [00:18<00:00, 17.22it/s, loss=0.0043, acc=0.987] \n",
      "100%|| 125/125 [00:01<00:00, 76.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       277\n",
      "           1       0.23      0.16      0.19       172\n",
      "           2       0.31      0.27      0.29       224\n",
      "           3       0.35      0.33      0.34       435\n",
      "           4       0.68      0.74      0.71       892\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.42      0.42      0.42      2000\n",
      "weighted avg       0.51      0.53      0.52      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[161  29  20  23  44]\n",
      " [ 62  28  29  22  31]\n",
      " [ 35  26  61  59  43]\n",
      " [ 14  24  59 142 196]\n",
      " [ 28  14  28 158 664]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 11):    \n",
    "    print('epoch', e)\n",
    "    model.train()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    with tqdm.tqdm(train_loader) as t:\n",
    "        for x, y in t:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += y.size(0)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': total_loss/total_count, 'acc': total_acc/total_count})\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with tqdm.tqdm(valid_loader) as t:\n",
    "        for x, y in t:\n",
    "            logits = model(x)\n",
    "            total_acc += (logits.argmax(1) == y).sum().item()\n",
    "            total_count += len(y)\n",
    "            y_pred += logits.argmax(1).tolist()\n",
    "            y_true += y.tolist()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning are full of tricks. \n",
    "\n",
    "In the second example above, the implementation of CNN is not good enough to beat even TFIDF+Logistic regression.\n",
    "\n",
    "You can use all the techniques introduced in the lectures and tutorials to enhance your methods.\n",
    "\n",
    "Of course, you can use ideas have not been mentioned to make your model distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
