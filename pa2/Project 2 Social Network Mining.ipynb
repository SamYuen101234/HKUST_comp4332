{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load networks into memory. Usually networks are organized as pairs of nodes. And sometimes different edges have different weights. Hence, we use networkx.DiGraph to store such structure information and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    read edges from an edge file\n",
    "    \"\"\"\n",
    "    edges = list()\n",
    "    df = pd.read_csv(file_name)\n",
    "    for idx, row in df.iterrows():\n",
    "        user_id, friends = row[\"user_id\"], eval(row[\"friends\"])\n",
    "        for friend in friends:\n",
    "            # add each friend relation as an edge\n",
    "            edges.append((user_id, friend))\n",
    "    edges = sorted(edges)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def load_test_data(file_name):\n",
    "    \"\"\"\n",
    "    read edges from an edge file\n",
    "    \"\"\"\n",
    "    edges = list()\n",
    "    scores = list()\n",
    "    df = pd.read_csv(file_name)\n",
    "    for idx, row in df.iterrows():\n",
    "        edges.append((row[\"src\"], row[\"dst\"]))\n",
    "    edges = sorted(edges)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def generate_false_edges(true_edges, num_false_edges=5):\n",
    "    \"\"\"\n",
    "    generate false edges given true edges\n",
    "    \"\"\"\n",
    "    nodes = list(set(chain.from_iterable(true_edges)))\n",
    "    N = len(nodes)\n",
    "    true_edges = set(true_edges)\n",
    "    print(N, len(true_edges))\n",
    "    false_edges = set()\n",
    "    \n",
    "    while len(false_edges) < num_false_edges:\n",
    "        # randomly sample two different nodes and check whether the pair exisit or not\n",
    "        src, dst = nodes[int(np.random.rand() * N)], nodes[int(np.random.rand() * N)]\n",
    "        if src != dst and (src, dst) not in true_edges and (src, dst) not in false_edges:\n",
    "            false_edges.add((src, dst))\n",
    "    false_edges = sorted(false_edges)\n",
    "    \n",
    "    return false_edges\n",
    "\n",
    "def construct_graph_from_edges(edges):\n",
    "    \"\"\"\n",
    "    generate a directed graph object given true edges\n",
    "    DiGraph documentation: https://networkx.github.io/documentation/stable/reference/classes/digraph.html\n",
    "    \"\"\"\n",
    "    # convert a list of edges {(u, v)} to a list of edges with weights {(u, v, w)}\n",
    "    edge_weight = defaultdict(float)\n",
    "    for e in edges:\n",
    "        edge_weight[e] += 1.0\n",
    "    weighed_edge_list = list()\n",
    "    for e in sorted(edge_weight.keys()):\n",
    "        weighed_edge_list.append((e[0], e[1], edge_weight[e]))\n",
    "        \n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_weighted_edges_from(weighed_edge_list)\n",
    "    \n",
    "    print(\"number of nodes:\", graph.number_of_nodes())\n",
    "    print(\"number of edges:\", graph.number_of_edges())\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random walk generators or random walkers yield random walks that contain both local and higher-order neighborhood information. However, naive non-uniform sampling is very slow, which requires O(n) time complexity. Here alias sampling can reduce the time complexity to O(1) with O(n) space. If you are interested, please see the following blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    \"\"\"\n",
    "    compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    details: https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    \"\"\"\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = list()\n",
    "    larger = list()\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K * prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def get_alias_node(graph, node):\n",
    "    \"\"\"\n",
    "    get the alias node setup lists for a given node.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the first-order information\n",
    "    unnormalized_probs = list()\n",
    "    for nbr in graph.neighbors(node):\n",
    "        unnormalized_probs.append(graph[node][nbr][\"weight\"])\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "        \n",
    "    return alias_setup(normalized_probs)\n",
    "    \n",
    "def get_alias_edge(graph, src, dst, p=1, q=1):\n",
    "    \"\"\"\n",
    "    get the alias edge setup lists for a given edge.\n",
    "    \"\"\"\n",
    "    # get the unnormalized probabilities with the second-order information\n",
    "    unnormalized_probs = list()\n",
    "    for dst_nbr in graph.neighbors(dst):\n",
    "        if dst_nbr == src: # distance is 0\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/p)\n",
    "        elif graph.has_edge(dst_nbr, src): # distance is 1\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"])\n",
    "        else: # distance is 2\n",
    "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/q)\n",
    "    unnormalized_probs = np.array(unnormalized_probs)\n",
    "    if len(unnormalized_probs) > 0:\n",
    "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
    "    else:\n",
    "        normalized_probs = unnormalized_probs\n",
    "\n",
    "    return alias_setup(normalized_probs)\n",
    "\n",
    "def preprocess_transition_probs(graph, p=1, q=1):\n",
    "    \"\"\"\n",
    "    preprocess transition probabilities for guiding the random walks.\n",
    "    \"\"\"\n",
    "    alias_nodes = dict()\n",
    "    for node in graph.nodes():\n",
    "        alias_nodes[node] = get_alias_node(graph, node)\n",
    "\n",
    "    alias_edges = dict()\n",
    "    for edge in graph.edges():\n",
    "        alias_edges[edge] = get_alias_edge(graph, edge[0], edge[1], p=p, q=q)\n",
    "\n",
    "    return alias_nodes, alias_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between DeepWalk and node2vec is how to generate random walks. The former only consider the first-order information while the latter also involves the second-order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    \"\"\"\n",
    "    draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    \"\"\"\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand() * K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "\n",
    "# helper function to generate the long random walk as desired\n",
    "def fallback(walk, fetch_last_num=1):\n",
    "    if len(walk) > fetch_last_num:\n",
    "        walk.pop()\n",
    "        fetched = []\n",
    "        for i in range(fetch_last_num):\n",
    "            fetched.append(walk[-1-i])\n",
    "        return walk, fetched\n",
    "    else:\n",
    "        return [], [None for _ in range(fetch_last_num)]\n",
    "\n",
    "def generate_first_order_random_walk(graph, alias_nodes, \n",
    "                                     walk_length=10, start_node=None, verbose=False, max_trails=10):\n",
    "    \"\"\"\n",
    "    simulate a random walk starting from start node and considering the first order information.\n",
    "    max_trials: set the max trials to be one for standard random walk. Larger max_trails will make the generated biased.\n",
    "    \"\"\"\n",
    "    if start_node == None:\n",
    "        start_node = np.random.choice(graph.nodes())\n",
    "    walk = [start_node]\n",
    "    cur = start_node\n",
    "    num_tried = 0\n",
    "    \n",
    "    ########## begin ##########\n",
    "    while len(walk) < walk_length:\n",
    "        cur_nbrs = list(graph.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0: # if we can sample next nodes\n",
    "            # sample the next node based on alias_nodes\n",
    "            cur = cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
    "            walk.append(cur)\n",
    "        else: # if we can't do that\n",
    "            num_tried += 1\n",
    "            if num_tried >= max_trails:\n",
    "                break\n",
    "\n",
    "            walk, fetched = fallback(walk, fetch_last_num=1)\n",
    "            cur = fetched[0]\n",
    "            if len(walk) == 0: # if falls back to the empty walk\n",
    "                start_node = np.random.choice(graph.nodes())\n",
    "                walk = [start_node]\n",
    "                cur = start_node\n",
    "    ########## end ##########\n",
    "\n",
    "    if verbose: \n",
    "        print(f'walk of lenght {len(walk)} generated with {num_tried} trails')\n",
    "    return walk\n",
    "    \n",
    "def generate_second_order_random_walk(graph, alias_nodes, alias_edges, \n",
    "                                      walk_length=10, start_node=None, verbose=False, max_trails=10):\n",
    "    \"\"\"\n",
    "    simulate a random walk starting from start node and considering the second order information.\n",
    "    \"\"\"\n",
    "    if start_node == None:\n",
    "        start_node = np.random.choice(graph.nodes())\n",
    "    walk = [start_node]\n",
    "    \n",
    "    prev = None\n",
    "    cur = start_node\n",
    "    num_tried = 0\n",
    "\n",
    "    ########## begin ##########\n",
    "    while len(walk) < walk_length:\n",
    "        cur_nbrs = list(graph.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if prev is None:\n",
    "                # sample the next node based on alias_nodes\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
    "            else:\n",
    "                # sample the next node based on alias_edges\n",
    "                prev, cur = cur, cur_nbrs[alias_draw(*alias_edges[(prev, cur)])]\n",
    "            walk.append(cur)\n",
    "        else:\n",
    "            num_tried += 1\n",
    "            if num_tried >= max_trails:\n",
    "                break\n",
    "            walk, (cur, prev) = fallback(walk, fetch_last_num=2)\n",
    "            if len(walk) == 0:\n",
    "                start_node = np.random.choice(graph.nodes())\n",
    "                walk = [start_node]\n",
    "                cur = start_node\n",
    "                prev = None\n",
    "    ########## end ##########\n",
    "    if verbose: \n",
    "        print(f'walk of lenght {len(walk)} generated with {num_tried} trails')\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Embedding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10):\n",
    "    \"\"\"\n",
    "    build a deepwalk model\n",
    "    \"\"\"\n",
    "    print(\"building a DeepWalk model...\", end=\"\\t\")\n",
    "    st = time.time()\n",
    "    np.random.seed(0)\n",
    "    nodes = list(graph.nodes())\n",
    "    walks = list()\n",
    "    # generate random walks\n",
    "    for walk_iter in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(generate_first_order_random_walk(\n",
    "                graph, alias_nodes, walk_length=walk_length, start_node=node))\n",
    "        \n",
    "    walk_lens = [len(w) for w in walks]\n",
    "    if len(walk_lens) > 0:\n",
    "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
    "    else:\n",
    "        avg_walk_len = 0.0\n",
    "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
    "    \n",
    "    # train a skip-gram model for these walks\n",
    "    model = Word2Vec(walks, vector_size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), epochs=10)\n",
    "    print(\"training time: %.4f\" % (time.time()-st))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10):\n",
    "    \"\"\"\n",
    "    build a node2vec model\n",
    "    \"\"\"\n",
    "    print(\"building a node2vec model...\", end=\"\\t\")\n",
    "    st = time.time()\n",
    "    np.random.seed(0)\n",
    "    nodes = list(graph.nodes())\n",
    "    walks = list()\n",
    "    # generate random walks\n",
    "    for walk_iter in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(generate_second_order_random_walk(\n",
    "                graph, alias_nodes, alias_edges, walk_length=walk_length, start_node=node))\n",
    "            \n",
    "    walk_lens = [len(w) for w in walks]\n",
    "    if len(walk_lens) > 0:\n",
    "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
    "    else:\n",
    "        avg_walk_len = 0.0    \n",
    "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
    "    \n",
    "    # train a skip-gram model for these walks\n",
    "    model = Word2Vec(walks, vector_size=node_dim, window=5, min_count=0, sg=1, workers=os.cpu_count(), epochs=20)\n",
    "    print(\"training time: %.4f\" % (time.time()-st))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(model, u, v):\n",
    "    \"\"\"\n",
    "    get the cosine similarity between two nodes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = model.wv[u]\n",
    "        v = model.wv[v]\n",
    "        return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def get_auc_score(model, true_edges, false_edges):\n",
    "    \"\"\"\n",
    "    get the auc score\n",
    "    \"\"\"\n",
    "    y_true = [1] * len(true_edges) + [0] * len(false_edges)\n",
    "    \n",
    "    y_score = list()\n",
    "    for e in true_edges:\n",
    "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
    "    for e in false_edges:\n",
    "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
    "    \n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "def write_pred(file_name, edges, scores):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"src\"] = [e[0] for e in edges]\n",
    "    df[\"dst\"] = [e[1] for e in edges]\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "def write_valid_ans(file_name, edges, scores):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"src\"] = [e[0] for e in edges]\n",
    "    df[\"dst\"] = [e[1] for e in edges]\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try them over a Real-life Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to load edges into memory and use the networkx.DiGraph structure to store the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 8328\n",
      "number of edges: 100000\n",
      "8474 119268\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "valid_file = \"data/valid.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "\n",
    "np.random.seed(0)\n",
    "train_edges = load_data(train_file)\n",
    "graph = construct_graph_from_edges(train_edges)\n",
    "valid_edges = load_data(valid_file)\n",
    "false_edges = generate_false_edges(train_edges+valid_edges, 40000-len(valid_edges))\n",
    "test_edges = load_test_data(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can use preprocess transition probabilities with the help of alias sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_nodes, alias_edges = preprocess_transition_probs(graph, p=1, q=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use random walk generators to generate random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate a first-order random walk and a second-order random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N6ZTMIue-2b30CJv2tyPGg',\n",
       " 'cvAPwZRWaDxSUudy8CR3Rw',\n",
       " '9E35LB29RXJixo0563Iwew',\n",
       " 'V74paj6Zok4DvGnciFEZMg',\n",
       " 'hiEQhuyMOSKCvVqmHrpvhg',\n",
       " 'V74paj6Zok4DvGnciFEZMg',\n",
       " 'rmw8xK8XEmo9yPPrIRbO5Q',\n",
       " 'tm-O-2D0mRt2mSwo6odOZw',\n",
       " 'TCPLaprJm4FFei2DkW6K6g',\n",
       " 'tm-O-2D0mRt2mSwo6odOZw']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_first_order_random_walk(graph, alias_nodes=alias_nodes,\n",
    "                                 start_node=\"N6ZTMIue-2b30CJv2tyPGg\", walk_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N6ZTMIue-2b30CJv2tyPGg',\n",
       " '3DYOjCqN4u47VV5ZPIeKiA',\n",
       " 'ACwN3HGetqPVhAgGKzdnsA',\n",
       " 'NsegE9Mqm3GCQeSx0v0tYw',\n",
       " 'EOxdQuedwe_2MkdLnX7MAA',\n",
       " '7sNE58P4AvsX6QHE8ypCiA',\n",
       " 'KQMzfaaI9jfo68S4CsCifA',\n",
       " 'pL4NhW3rTokFr-CQh3Lzgg',\n",
       " 'G-_KF_Ul4d3WGEa-G0Iq4g',\n",
       " 'PAeEkjrXTub0ENa4rZiWvA']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_second_order_random_walk(graph, alias_nodes=alias_nodes, alias_edges=alias_edges,\n",
    "                                  start_node=\"N6ZTMIue-2b30CJv2tyPGg\", walk_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can build a DeepWalk model and a node2vec model. Here we set p=q=0.5 so that the walker will not go very far away from the start node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building a DeepWalk model...\tnumber of walks: 83280\taverage walk length: 9.8980\ttraining time: 17.0101\n"
     ]
    }
   ],
   "source": [
    "model = build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 9.9910\ttraining time: 32.5295\n"
     ]
    }
   ],
   "source": [
    "model = build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the node embeddings of three nodes, and cosine similarities of two edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node embedding (\"N6ZTMIue-2b30CJv2tyPGg\"): [ 0.3806834  -0.16270779  2.5158856   0.72005105 -0.99909395  0.6211217\n",
      "  0.01078082  0.58799267 -0.46484518 -0.6557571 ]\n",
      "node embedding (\"N7E-CfqdME28dakWdEKNvw\"): [ 0.60011697 -0.10568868  1.3504603   0.35519686 -1.0511355   0.6129129\n",
      "  0.9517697   0.16542582  0.13488191 -0.54862654]\n",
      "node embedding (\"MmlJSLDg-IFaeXb5wdJbgg\"): [ 1.1668385  -0.01565974 -0.91542506 -0.15480001 -0.199291    1.7392436\n",
      "  2.5477452   1.1600933   0.18508358 -1.5321856 ]\n",
      "true edge (\"N6ZTMIue-2b30CJv2tyPGg\", \"N7E-CfqdME28dakWdEKNvw\"): 0.83216554\n",
      "false edge (\"N6ZTMIue-2b30CJv2tyPGg\", \"MmlJSLDg-IFaeXb5wdJbgg\"): 0.07786497\n"
     ]
    }
   ],
   "source": [
    "print(\"node embedding (\\\"N6ZTMIue-2b30CJv2tyPGg\\\"):\",\n",
    "      model.wv[\"N6ZTMIue-2b30CJv2tyPGg\"])\n",
    "print(\"node embedding (\\\"N7E-CfqdME28dakWdEKNvw\\\"):\",\n",
    "      model.wv[\"N7E-CfqdME28dakWdEKNvw\"])\n",
    "print(\"node embedding (\\\"MmlJSLDg-IFaeXb5wdJbgg\\\"):\",\n",
    "      model.wv.vectors[model.wv.index_to_key.index(\"MmlJSLDg-IFaeXb5wdJbgg\")])\n",
    "print(\"true edge (\\\"N6ZTMIue-2b30CJv2tyPGg\\\", \\\"N7E-CfqdME28dakWdEKNvw\\\"):\",\n",
    "      get_cosine_sim(model, \"N6ZTMIue-2b30CJv2tyPGg\", \"N7E-CfqdME28dakWdEKNvw\"))\n",
    "print(\"false edge (\\\"N6ZTMIue-2b30CJv2tyPGg\\\", \\\"MmlJSLDg-IFaeXb5wdJbgg\\\"):\",\n",
    "      get_cosine_sim(model, \"N6ZTMIue-2b30CJv2tyPGg\", \"MmlJSLDg-IFaeXb5wdJbgg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model with different parameters and test the model on the validation set. Please show your parameter search process and analysis in your code, and visualization tools (Heatmap, etc.) are recommended to make your analysis clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node dim: 10,\tnum_walks: 10,\twalk_length: 10\tbuilding a DeepWalk model...\tnumber of walks: 83280\taverage walk length: 9.8980\ttraining time: 18.1615\n",
      "auc: 0.9253\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "node_dim = 10\n",
    "num_walks = 10\n",
    "walk_length = 10\n",
    "\n",
    "deepwalk_auc_scores = dict()\n",
    "\n",
    "#alias_nodes, alias_edges = preprocess_transition_probs(graph, p=0.5, q=0.5)\n",
    "print(\"node dim: %d,\\tnum_walks: %d,\\twalk_length: %d\" % (node_dim, num_walks, walk_length), end=\"\\t\")\n",
    "model = build_deepwalk(graph, alias_nodes, \n",
    "                       node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "deepwalk_auc_scores[(node_dim, num_walks, walk_length)] = get_auc_score(model, valid_edges, false_edges)\n",
    "print(\"auc: %.4f\" % (deepwalk_auc_scores[(node_dim, num_walks, walk_length)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node dim: 10,\tnum_walks: 40,\twalk_length: 15,\tp: 0.90,\tq: 0.70\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 14.9800\ttraining time: 174.6017\n",
      "auc: 0.9295\n",
      "node dim: 15,\tnum_walks: 30,\twalk_length: 30,\tp: 0.20,\tq: 0.70\tbuilding a node2vec model...\tnumber of walks: 249840\taverage walk length: 29.9496\ttraining time: 270.6347\n",
      "auc: 0.9249\n",
      "node dim: 20,\tnum_walks: 40,\twalk_length: 10,\tp: 0.40,\tq: 0.90\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 9.9914\ttraining time: 110.5229\n",
      "auc: 0.9263\n",
      "node dim: 15,\tnum_walks: 25,\twalk_length: 30,\tp: 0.60,\tq: 0.70\tbuilding a node2vec model...\tnumber of walks: 208200\taverage walk length: 29.9340\ttraining time: 197.5651\n",
      "auc: 0.9270\n",
      "node dim: 10,\tnum_walks: 20,\twalk_length: 10,\tp: 0.30,\tq: 0.50\tbuilding a node2vec model...\tnumber of walks: 166560\taverage walk length: 9.9911\ttraining time: 51.0930\n",
      "auc: 0.9313\n",
      "node dim: 20,\tnum_walks: 40,\twalk_length: 30,\tp: 0.60,\tq: 0.70\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 29.9335\ttraining time: 300.1573\n",
      "auc: 0.9251\n",
      "node dim: 15,\tnum_walks: 40,\twalk_length: 25,\tp: 0.80,\tq: 0.60\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 24.9463\ttraining time: 260.2991\n",
      "auc: 0.9276\n",
      "node dim: 15,\tnum_walks: 10,\twalk_length: 20,\tp: 0.60,\tq: 0.90\tbuilding a node2vec model...\tnumber of walks: 83280\taverage walk length: 19.9720\ttraining time: 49.1125\n",
      "auc: 0.9323\n",
      "node dim: 10,\tnum_walks: 15,\twalk_length: 30,\tp: 0.40,\tq: 0.90\tbuilding a node2vec model...\tnumber of walks: 124920\taverage walk length: 29.9481\ttraining time: 112.4456\n",
      "auc: 0.9290\n",
      "node dim: 15,\tnum_walks: 20,\twalk_length: 35,\tp: 0.70,\tq: 0.40\tbuilding a node2vec model...\tnumber of walks: 166560\taverage walk length: 34.8805\ttraining time: 179.3107\n",
      "auc: 0.9275\n",
      "node dim: 20,\tnum_walks: 40,\twalk_length: 35,\tp: 0.30,\tq: 0.40\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 34.9114\ttraining time: 348.4043\n",
      "auc: 0.9219\n",
      "node dim: 10,\tnum_walks: 10,\twalk_length: 20,\tp: 0.80,\tq: 0.90\tbuilding a node2vec model...\tnumber of walks: 83280\taverage walk length: 19.9668\ttraining time: 46.8913\n",
      "auc: 0.9316\n",
      "node dim: 10,\tnum_walks: 40,\twalk_length: 15,\tp: 0.90,\tq: 0.40\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 14.9737\ttraining time: 162.9643\n",
      "auc: 0.9300\n",
      "node dim: 20,\tnum_walks: 40,\twalk_length: 35,\tp: 0.90,\tq: 0.20\tbuilding a node2vec model...\tnumber of walks: 333120\taverage walk length: 34.8379\ttraining time: 350.0366\n",
      "auc: 0.9245\n",
      "node dim: 15,\tnum_walks: 15,\twalk_length: 20,\tp: 0.60,\tq: 0.50\tbuilding a node2vec model...\tnumber of walks: 124920\taverage walk length: 19.9651\ttraining time: 74.3846\n",
      "auc: 0.9305\n",
      "node dim: 15,\tnum_walks: 20,\twalk_length: 10,\tp: 0.50,\tq: 0.50\tbuilding a node2vec model...\tnumber of walks: 166560\taverage walk length: 9.9915\ttraining time: 52.2123\n",
      "auc: 0.9313\n",
      "node dim: 20,\tnum_walks: 10,\twalk_length: 30,\tp: 0.40,\tq: 0.30\tbuilding a node2vec model...\tnumber of walks: 83280\taverage walk length: 29.9192\ttraining time: 71.9762\n",
      "auc: 0.9262\n",
      "node dim: 20,\tnum_walks: 20,\twalk_length: 10,\tp: 0.20,\tq: 0.90\tbuilding a node2vec model...\tnumber of walks: 166560\taverage walk length: 9.9919\ttraining time: 50.3404\n",
      "auc: 0.9267\n",
      "node dim: 20,\tnum_walks: 25,\twalk_length: 30,\tp: 0.80,\tq: 0.80\tbuilding a node2vec model...\tnumber of walks: 208200\taverage walk length: 29.9313\ttraining time: 185.2550\n",
      "auc: 0.9245\n",
      "node dim: 20,\tnum_walks: 25,\twalk_length: 30,\tp: 0.80,\tq: 0.70\tbuilding a node2vec model...\tnumber of walks: 208200\taverage walk length: 29.9257\ttraining time: 184.4289\n",
      "auc: 0.9243\n",
      "Best auc: 0.9323\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "\n",
    "node_dim_list = [10, 15, 20]\n",
    "num_walks_list = [10, 15, 20, 25, 30, 40]\n",
    "walk_length_list = [10, 15, 20, 25, 30, 35]\n",
    "p_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "q_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "epochs = 20\n",
    "best_auc = 0\n",
    "\n",
    "node2vec_auc_scores = dict()\n",
    "for _ in range(epochs):\n",
    "    node_dim = random.choice(node_dim_list)\n",
    "    num_walks = random.choice(num_walks_list)\n",
    "    walk_length = random.choice(walk_length_list)\n",
    "    p = random.choice(p_list)\n",
    "    q = random.choice(q_list)\n",
    "    print(\"node dim: %d,\\tnum_walks: %d,\\twalk_length: %d,\\tp: %.2f,\\tq: %.2f\" % (\n",
    "        node_dim, num_walks, walk_length, p, q), end=\"\\t\")\n",
    "    alias_nodes, alias_edges = preprocess_transition_probs(graph, p=p, q=q)\n",
    "    model = build_node2vec(graph, alias_nodes, alias_edges, \n",
    "                           node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
    "    node2vec_auc_scores[(node_dim, num_walks, walk_length, p, q)] = get_auc_score(model, valid_edges, false_edges)\n",
    "    print(\"auc: %.4f\" % (node2vec_auc_scores[(node_dim, num_walks, walk_length, p, q)]))\n",
    "    if (node2vec_auc_scores[(node_dim, num_walks, walk_length, p, q)]) > best_auc:\n",
    "        best_auc = (node2vec_auc_scores[(node_dim, num_walks, walk_length, p, q)])\n",
    "\n",
    "print(\"Best auc: %.4f\" % best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using Heatmap to viualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(10, 5, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-9b8005ddd897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnode_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# you should have an auc score dictionary here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepwalk_auc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwalk_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_walks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#plt.colorbar()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-9b8005ddd897>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnode_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# you should have an auc score dictionary here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepwalk_auc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwalk_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_walks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#plt.colorbar()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-9b8005ddd897>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnode_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# you should have an auc score dictionary here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepwalk_auc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwalk_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_walks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#plt.colorbar()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (10, 5, 10)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "node_dim = 20\n",
    "# you should have an auc score dictionary here.\n",
    "a = np.array([[deepwalk_auc_scores[(node_dim, num_walks, walk_length)] for walk_length in [10, 20, 40]] for num_walks in [5, 10, 20, 40]])\n",
    "plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks=[0,1,2], labels=[10, 20, 40])\n",
    "plt.xlabel(\"walk_length\")\n",
    "plt.yticks(ticks=[0,1,2,3], labels=[5, 10, 20, 40])\n",
    "plt.ylabel(\"num_walks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build model with the best parameters you find and save the prediction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building a DeepWalk model...\tnumber of walks: 83280\taverage walk length: 9.8980\ttrainig time: 37.0635\n"
     ]
    }
   ],
   "source": [
    "model = build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10)\n",
    "scores = [get_cosine_sim(model, src, dst) for src, dst in test_edges]\n",
    "write_pred(\"data/pred.csv\", test_edges, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of p and q, the node2vec model can fit training data better. And you can have a try if you set p=q=1, the two models will return the same results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
