{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #comment out this line if you want to use gpu\n",
    "import random\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "# tf > 2.0\n",
    "tensorflow.random.set_seed(2021)\n",
    "#tf < 2.0\n",
    "#tf.set_random_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE) is used to evaluate the performance of a recommendation algorithm, so we need to define the following utility function to compute the RMSE given the predicted ratings and the ground truth ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -pred: an array containing all predicted ratings\n",
    "    -actual: an array containing all ground truth ratings\n",
    "    \n",
    "return:\n",
    "    a scalar whose value is the rmse\n",
    "'''\n",
    "def rmse(pred, actual):\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Wide and Deep Learning (WDL) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The wide component is a generalized linear model that takes in the raw input features and the cross-product transformation of categorical features, which enables it to learn the frequent co-occurrence of items or features. \n",
    "\n",
    "### The deep component is a Feed-forward Neural Network (FNN) which takes in both continuous and categorical features as input. Specifically,  the normalized values of continuous features are concatenated with the low-dimensional dense embedding vectors converted from categorical features. This concatenated vector is then fed into the FNN during each foward pass. This mechanism tend to increase the diversity of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -len_continuous: number of continuous features\n",
    "    -deep_vocab_lens: an array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep \n",
    "        categorical feature\n",
    "    -len_wide: number of wide features\n",
    "    -embed_size: dimension of the embedding vectors of deep categorical features\n",
    "    \n",
    "return:\n",
    "    a keras Model object for the constructed wdl model \n",
    "'''\n",
    "\n",
    "\n",
    "def build_wdl_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    # A list containing all input layers\n",
    "    input_list = []\n",
    "    \n",
    "    # Input layer for continuous features\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "    \n",
    "    \n",
    "    # Get embeddings for all deep categorical features\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Create input layer for deep component by concatenating the embeddings and continuous features' input layer\n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "    \n",
    "\n",
    "    # Construct deep component\n",
    "    dense_1 = Dense(256, activation='relu')(deep_input)\n",
    "    dense_1_dp = Dropout(0.3)(dense_1)\n",
    "    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n",
    "    dense_2_dp = Dropout(0.3)(dense_2)\n",
    "    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n",
    "    dense_3_dp = Dropout(0.3)(dense_3)\n",
    "\n",
    "    \n",
    "    # Create input layer for wide component\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "\n",
    "    \n",
    "    # Concatenate the outputs of deep and wide components and feed the \n",
    "    # concatenated vector into the finall fully connected layer\n",
    "    fc_input = Concatenate()([dense_3_dp, wide_input])\n",
    "    model_output = Dense(1)(fc_input)\n",
    "    \n",
    "    model = Model(inputs=input_list,\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions to get the values of different types of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -continuous_columns: column names of continuous features\n",
    "    \n",
    "return: \n",
    "    a numpy array where each row contains the values of continuous features in the corresponding row of the\n",
    "    input dataframe\n",
    "'''\n",
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross product transformation of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -comb_p: number of elements in each combination (e.g., there are two elements in the combination {fried chicken, chicken and \n",
    "    waffle}, and three elements in the combination {fried chicken, chicken and waffle, chicken fried rice})\n",
    "    -topk: number of mostly frequent combinations to retrieve\n",
    "    -output_freq: whether to return the frequencies of retrieved combinations\n",
    "    \n",
    "return:\n",
    "    1. output_freq = True: a list X where each element is a tuple containing a combinantion tuple and corresponding frequency, and the \n",
    "        elements are stored in the descending order of their frequencies\n",
    "    2. output_freq = False: a list X where each element is a tuple containing a combinantion tuple, and the elements are stored in \n",
    "    the descending order of their frequencies\n",
    "'''\n",
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    # get all combinations with comb_p\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    # [('Lounges', 'Dance Clubs'), ('Lounges', 'Bars'), ('Lounges', 'Nightlife'), ('Dance Clubs', 'Bars'), ('Dance Clubs', 'Nightlife'), ('Bars', 'Nightlife')]\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    # ('Lounges', 'Dance Clubs')\n",
    "    # list of tuples that each index refer to one combination\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -selected_categories_to_idx: a dictionary mapping item categories to corrresponding integral indices\n",
    "    -top_combinations: a list containing retrieved mostly frequent combinantions of item categories\n",
    "    \n",
    "return:\n",
    "    a numpy array where each row contains the categorical features' binary encodings and cross product\n",
    "    transformations for the corresponding row of the input dataframe\n",
    "'''\n",
    "\n",
    "def get_wide_features(df, selected_categories_to_idx, top_combinations):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    print('category_binary_features shape:',category_binary_features.shape)\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    print('category_cross_features shape:',category_corss_transform_features.shape)\n",
    "    out = np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n",
    "    print('wide features shape:',out.shape)\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, validation and test rating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")\n",
    "te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load content feautures tables of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"data/user.csv\")\n",
    "item_df = pd.read_csv(\"data/business.csv\")\n",
    "\n",
    "# Rename some columns of dfs and convert the indices of dfs into string type for easier reference in later stage \n",
    "user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate each row in the rating tables with corresponding user's and item's content features through merging the rating tables and content features tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original row indices of each rating table\n",
    "tr_df[\"index\"] = tr_df.index\n",
    "val_df[\"index\"]  = val_df.index\n",
    "te_df[\"index\"] = te_df.index\n",
    "\n",
    "tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>index</th>\n",
       "      <th>user_Unnamed: 0</th>\n",
       "      <th>user_average_stars</th>\n",
       "      <th>user_compliment_cool</th>\n",
       "      <th>user_compliment_cute</th>\n",
       "      <th>user_compliment_funny</th>\n",
       "      <th>user_compliment_hot</th>\n",
       "      <th>...</th>\n",
       "      <th>item_city</th>\n",
       "      <th>item_hours</th>\n",
       "      <th>item_is_open</th>\n",
       "      <th>item_latitude</th>\n",
       "      <th>item_longitude</th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_postal_code</th>\n",
       "      <th>item_review_count</th>\n",
       "      <th>item_stars</th>\n",
       "      <th>item_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ec8f38aa91755dcf5837020d022ad384</td>\n",
       "      <td>ecaa90564e18dca1c7b653038f71d6bf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3846</td>\n",
       "      <td>3.13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '22:0-4:0', 'Friday': '22:0-4:0', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.128050</td>\n",
       "      <td>-115.164869</td>\n",
       "      <td>XS Nightclub</td>\n",
       "      <td>89109</td>\n",
       "      <td>3055</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64fe4dd0a489c9b96a3e8d7fbd337888</td>\n",
       "      <td>ef118bb0ae1fc369e1f47d1b34f6acee</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1596</td>\n",
       "      <td>4.24</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.014838</td>\n",
       "      <td>-115.171263</td>\n",
       "      <td>Yanni's Greek Grill</td>\n",
       "      <td>89123</td>\n",
       "      <td>390</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a49909b39426ebb3538aa837b5b88840</td>\n",
       "      <td>e8b182a923810d52981aa02d56dde799</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4095</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{'Tuesday': '15:0-21:0', 'Wednesday': '15:0-21...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.521715</td>\n",
       "      <td>-112.064124</td>\n",
       "      <td>Fuego Bistro</td>\n",
       "      <td>85014</td>\n",
       "      <td>684</td>\n",
       "      <td>4.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a56726d5676d647e42e2aca54f21b075</td>\n",
       "      <td>250040e979eae9ef5912aa5a1d285e4e</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4631</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '10:0-18:0', 'Tuesday': '10:0-18:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.114062</td>\n",
       "      <td>-115.177260</td>\n",
       "      <td>Cirque du Soleil - O</td>\n",
       "      <td>89109</td>\n",
       "      <td>1519</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3e19d8260e655ba87bea0922bac92266</td>\n",
       "      <td>e02880faf4d42fe1df7bd370fb1c787b</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>949</td>\n",
       "      <td>4.02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>36.068171</td>\n",
       "      <td>-115.172363</td>\n",
       "      <td>Town Square Las Vegas</td>\n",
       "      <td>89119</td>\n",
       "      <td>543</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>555883855cca31f06edb517762bc8171</td>\n",
       "      <td>396739eaa8b6cbfd078628567f1f01cf</td>\n",
       "      <td>5.0</td>\n",
       "      <td>99995</td>\n",
       "      <td>4158</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Scottsdale</td>\n",
       "      <td>{'Monday': '12:0-18:0', 'Tuesday': '12:0-22:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.583703</td>\n",
       "      <td>-111.883739</td>\n",
       "      <td>Sweet Republic</td>\n",
       "      <td>85260</td>\n",
       "      <td>717</td>\n",
       "      <td>4.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>634d0478d05af1775d606058521593dc</td>\n",
       "      <td>1bfdf72aac7cbe0fc94caeb4422b629d</td>\n",
       "      <td>4.0</td>\n",
       "      <td>99996</td>\n",
       "      <td>2424</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>{'Monday': '16:30-21:0', 'Tuesday': '16:30-21:...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.377686</td>\n",
       "      <td>-111.752849</td>\n",
       "      <td>Thai Chili</td>\n",
       "      <td>85234</td>\n",
       "      <td>460</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0f37c0d13690b022a55470894714afd6</td>\n",
       "      <td>78a3e9835377387d0a04664abec56143</td>\n",
       "      <td>4.0</td>\n",
       "      <td>99997</td>\n",
       "      <td>718</td>\n",
       "      <td>3.91</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...</td>\n",
       "      <td>0</td>\n",
       "      <td>35.152611</td>\n",
       "      <td>-80.828543</td>\n",
       "      <td>Chuy's</td>\n",
       "      <td>28211</td>\n",
       "      <td>211</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>54e7e0b4b4a95c6a8a979d24d383e39d</td>\n",
       "      <td>0b0ffffc5987e32fd87dd4782bb75eb1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>99998</td>\n",
       "      <td>1865</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '6:30-22:30', 'Tuesday': '6:30-22:3...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.227991</td>\n",
       "      <td>-80.843388</td>\n",
       "      <td>Stoke Charlotte</td>\n",
       "      <td>28202</td>\n",
       "      <td>154</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1369628397cbb9a2fe6c55427e2c6466</td>\n",
       "      <td>13b3d6270f1d98ea4a6ff7cacb84f174</td>\n",
       "      <td>5.0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1212</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{'Monday': '9:0-15:0', 'Tuesday': '9:0-21:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.496386</td>\n",
       "      <td>-112.083983</td>\n",
       "      <td>PHX Burrito House</td>\n",
       "      <td>85013</td>\n",
       "      <td>556</td>\n",
       "      <td>4.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id                       business_id  \\\n",
       "0      ec8f38aa91755dcf5837020d022ad384  ecaa90564e18dca1c7b653038f71d6bf   \n",
       "1      64fe4dd0a489c9b96a3e8d7fbd337888  ef118bb0ae1fc369e1f47d1b34f6acee   \n",
       "2      a49909b39426ebb3538aa837b5b88840  e8b182a923810d52981aa02d56dde799   \n",
       "3      a56726d5676d647e42e2aca54f21b075  250040e979eae9ef5912aa5a1d285e4e   \n",
       "4      3e19d8260e655ba87bea0922bac92266  e02880faf4d42fe1df7bd370fb1c787b   \n",
       "...                                 ...                               ...   \n",
       "99995  555883855cca31f06edb517762bc8171  396739eaa8b6cbfd078628567f1f01cf   \n",
       "99996  634d0478d05af1775d606058521593dc  1bfdf72aac7cbe0fc94caeb4422b629d   \n",
       "99997  0f37c0d13690b022a55470894714afd6  78a3e9835377387d0a04664abec56143   \n",
       "99998  54e7e0b4b4a95c6a8a979d24d383e39d  0b0ffffc5987e32fd87dd4782bb75eb1   \n",
       "99999  1369628397cbb9a2fe6c55427e2c6466  13b3d6270f1d98ea4a6ff7cacb84f174   \n",
       "\n",
       "       stars  index  user_Unnamed: 0  user_average_stars  \\\n",
       "0        1.0      0             3846                3.13   \n",
       "1        5.0      1             1596                4.24   \n",
       "2        5.0      2             4095                4.33   \n",
       "3        5.0      3             4631                4.60   \n",
       "4        4.0      4              949                4.02   \n",
       "...      ...    ...              ...                 ...   \n",
       "99995    5.0  99995             4158                3.42   \n",
       "99996    4.0  99996             2424                3.26   \n",
       "99997    4.0  99997              718                3.91   \n",
       "99998    4.0  99998             1865                3.34   \n",
       "99999    5.0  99999             1212                2.96   \n",
       "\n",
       "       user_compliment_cool  user_compliment_cute  user_compliment_funny  \\\n",
       "0                        20                     1                     20   \n",
       "1                        20                     1                     20   \n",
       "2                         2                     0                      2   \n",
       "3                         0                     0                      0   \n",
       "4                         2                     0                      2   \n",
       "...                     ...                   ...                    ...   \n",
       "99995                     0                     0                      0   \n",
       "99996                     2                     0                      2   \n",
       "99997                     5                     0                      5   \n",
       "99998                     0                     0                      0   \n",
       "99999                     2                     1                      2   \n",
       "\n",
       "       user_compliment_hot  ...   item_city  \\\n",
       "0                        5  ...   Las Vegas   \n",
       "1                        9  ...   Las Vegas   \n",
       "2                        1  ...     Phoenix   \n",
       "3                        0  ...   Las Vegas   \n",
       "4                        0  ...   Las Vegas   \n",
       "...                    ...  ...         ...   \n",
       "99995                    0  ...  Scottsdale   \n",
       "99996                    0  ...     Gilbert   \n",
       "99997                   11  ...   Charlotte   \n",
       "99998                    0  ...   Charlotte   \n",
       "99999                    1  ...     Phoenix   \n",
       "\n",
       "                                              item_hours  item_is_open  \\\n",
       "0      {'Monday': '22:0-4:0', 'Friday': '22:0-4:0', '...             1   \n",
       "1      {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...             1   \n",
       "2      {'Tuesday': '15:0-21:0', 'Wednesday': '15:0-21...             1   \n",
       "3      {'Monday': '10:0-18:0', 'Tuesday': '10:0-18:0'...             1   \n",
       "4                                                    NaN             1   \n",
       "...                                                  ...           ...   \n",
       "99995  {'Monday': '12:0-18:0', 'Tuesday': '12:0-22:0'...             1   \n",
       "99996  {'Monday': '16:30-21:0', 'Tuesday': '16:30-21:...             1   \n",
       "99997  {'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...             0   \n",
       "99998  {'Monday': '6:30-22:30', 'Tuesday': '6:30-22:3...             1   \n",
       "99999  {'Monday': '9:0-15:0', 'Tuesday': '9:0-21:0', ...             1   \n",
       "\n",
       "       item_latitude  item_longitude              item_name  item_postal_code  \\\n",
       "0          36.128050     -115.164869           XS Nightclub             89109   \n",
       "1          36.014838     -115.171263    Yanni's Greek Grill             89123   \n",
       "2          33.521715     -112.064124           Fuego Bistro             85014   \n",
       "3          36.114062     -115.177260   Cirque du Soleil - O             89109   \n",
       "4          36.068171     -115.172363  Town Square Las Vegas             89119   \n",
       "...              ...             ...                    ...               ...   \n",
       "99995      33.583703     -111.883739         Sweet Republic             85260   \n",
       "99996      33.377686     -111.752849             Thai Chili             85234   \n",
       "99997      35.152611      -80.828543                 Chuy's             28211   \n",
       "99998      35.227991      -80.843388        Stoke Charlotte             28202   \n",
       "99999      33.496386     -112.083983      PHX Burrito House             85013   \n",
       "\n",
       "       item_review_count item_stars  item_state  \n",
       "0                   3055        4.0          NV  \n",
       "1                    390        4.5          NV  \n",
       "2                    684        4.5          AZ  \n",
       "3                   1519        4.5          NV  \n",
       "4                    543        4.0          NV  \n",
       "...                  ...        ...         ...  \n",
       "99995                717        4.5          AZ  \n",
       "99996                460        4.0          AZ  \n",
       "99997                211        3.5          NC  \n",
       "99998                154        4.0          NC  \n",
       "99999                556        4.5          AZ  \n",
       "\n",
       "[100000 rows x 39 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns containing conitnuous features\n",
    "continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n",
    "                      \"user_review_count\", \"user_useful\", \"user_funny\",\n",
    "                      \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
    "                      \"item_review_count\", \"item_stars\"]\n",
    "\n",
    "# Get values of continous features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "\n",
    "# Standardize each feature by removing the mean of the training samples and scaling to unit variance.\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for more details.\n",
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare deep categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcify column names of deep categorical features\n",
    "item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "\n",
    "# An array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n",
    "item_deep_vocab_lens = []\n",
    "\n",
    "for col_name in item_deep_columns:\n",
    "    # Get all unique values of this deep categorical feature\n",
    "    tmp = item_df[col_name].unique()\n",
    "    \n",
    "    # Create a dictionary mapping each unique value to a unique integral index\n",
    "    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "    \n",
    "    # Get the number of unique values of this deep categorical features\n",
    "    item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "    \n",
    "    # Create a new column where each entry stores the integral index of this deep categorical feature's value in the same row\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x])\n",
    "\n",
    "\n",
    "# Create a dictionary mapping each business id to corresponding values of deep categorical features\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_categorical_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "\n",
    "# Creat numpy arrays storing corresponding deep categorical features' values of train/validation/test sets using the above mapping\n",
    "tr_deep_categorical_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "val_deep_categorical_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "te_deep_categorical_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare wide features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare binary encoding for each selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categories of all items \n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "\n",
    "# Sort all unique values of the item categories by their frequencies in descending order\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top 500 most frequent categories\n",
    "selected_categories = [t[0] for t in category_sorted[:500]]\n",
    "\n",
    "# Create a dictionary mapping each secleted category to a unique integral index\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "\n",
    "# Map all categories unseen in the item df to index 0\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "\n",
    "# Create a dictionary mapping each integral index to corresponding category\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare cross product transformation for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent categories combinantions using the utility function defined previously and store them in the folloing list\n",
    "top_combinations = []\n",
    "\n",
    "# Get top 50 most frequent two-categories combinantions in the train set\n",
    "\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n",
    "\n",
    "# Get top 30 most frequent three-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
    "\n",
    "# Get top 20 most frequent four-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
    "\n",
    "# Convert each combinantion in the list to a set data structure\n",
    "top_combinations = [set(t) for t in top_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_binary_features shape: (100000, 501)\n",
      "category_cross_features shape: (100000, 100)\n",
      "wide features shape: (100000, 601)\n",
      "category_binary_features shape: (10000, 501)\n",
      "category_cross_features shape: (10000, 100)\n",
      "wide features shape: (10000, 601)\n",
      "category_binary_features shape: (10000, 501)\n",
      "category_cross_features shape: (10000, 100)\n",
      "wide features shape: (10000, 601)\n"
     ]
    }
   ],
   "source": [
    "# Get values of wide features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_wide_features = get_wide_features(tr_df, selected_categories_to_idx, top_combinations)\n",
    "val_wide_features = get_wide_features(val_df, selected_categories_to_idx, top_combinations)\n",
    "te_wide_features = get_wide_features(te_df, selected_categories_to_idx, top_combinations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the input list for each of the train/validation/test sets through aggregating all continuous, deep categorical and wide features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_features = [tr_continuous_features,categorical_features_0,categorical_features_1,categorical_features_2,tr_wide_features]\n",
    "tr_features = []\n",
    "tr_features.append(tr_continuous_features.tolist())\n",
    "tr_features += [tr_deep_categorical_features[:,i].tolist() for i in range(tr_deep_categorical_features.shape[1])]\n",
    "tr_features.append(tr_wide_features.tolist())\n",
    "\n",
    "\n",
    "\n",
    "val_features = []\n",
    "val_features.append(val_continuous_features.tolist())\n",
    "val_features += [val_deep_categorical_features[:,i].tolist() for i in range(val_deep_categorical_features.shape[1])]\n",
    "val_features.append(val_wide_features.tolist())\n",
    "\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features.tolist())\n",
    "te_features += [te_deep_categorical_features[:,i].tolist() for i in range(te_deep_categorical_features.shape[1])]\n",
    "te_features.append(te_wide_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the WDL model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdl_model = build_wdl_model(\n",
    "        len(tr_continuous_features[0]),\n",
    "        item_deep_vocab_lens,   # num of category classes\n",
    "        len(tr_wide_features[0]), \n",
    "        embed_size=100)\n",
    "#print(len(tr_continuous_features[0]))\n",
    "#print(item_deep_vocab_lens)\n",
    "#print(len(tr_wide_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using Adagrad optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-14cc28f63cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtr_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtr_ratings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         epochs=1, verbose=1, callbacks=[ModelCheckpoint('model.h5')])\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "wdl_model.compile(optimizer='adagrad', loss='mse')\n",
    "\n",
    "history = wdl_model.fit(\n",
    "        tr_features, \n",
    "        tr_ratings, \n",
    "        epochs=1, verbose=1, callbacks=[ModelCheckpoint('model.h5')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on train and validation sets using RMSE¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  1.039801672098216\n",
      "VALID RMSE:  1.0475014041928454\n"
     ]
    }
   ],
   "source": [
    "y_pred = wdl_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = wdl_model.predict(val_features)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
