{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #comment out this line if you want to use gpu\n",
    "import random\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "# tf > 2.0\n",
    "tensorflow.random.set_seed(2021)\n",
    "#tf < 2.0\n",
    "#tf.set_random_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE) is used to evaluate the performance of a recommendation algorithm, so we need to define the following utility function to compute the RMSE given the predicted ratings and the ground truth ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -pred: an array containing all predicted ratings\n",
    "    -actual: an array containing all ground truth ratings\n",
    "    \n",
    "return:\n",
    "    a scalar whose value is the rmse\n",
    "'''\n",
    "def rmse(pred, actual):\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Wide and Deep Learning (WDL) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The wide component is a generalized linear model that takes in the raw input features and the cross-product transformation of categorical features, which enables it to learn the frequent co-occurrence of items or features. \n",
    "\n",
    "### The deep component is a Feed-forward Neural Network (FNN) which takes in both continuous and categorical features as input. Specifically,  the normalized values of continuous features are concatenated with the low-dimensional dense embedding vectors converted from categorical features. This concatenated vector is then fed into the FNN during each foward pass. This mechanism tend to increase the diversity of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -len_continuous: number of continuous features\n",
    "    -deep_vocab_lens: an array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep \n",
    "        categorical feature\n",
    "    -len_wide: number of wide features\n",
    "    -embed_size: dimension of the embedding vectors of deep categorical features\n",
    "    \n",
    "return:\n",
    "    a keras Model object for the constructed wdl model \n",
    "'''\n",
    "\n",
    "\n",
    "def build_wdl_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    # A list containing all input layers\n",
    "    input_list = []\n",
    "    \n",
    "    # Input layer for continuous features\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "    \n",
    "    \n",
    "    # Get embeddings for all deep categorical features\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Create input layer for deep component by concatenating the embeddings and continuous features' input layer\n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "    \n",
    "\n",
    "    # Construct deep component\n",
    "    dense_1 = Dense(256, activation='relu')(deep_input)\n",
    "    dense_1_dp = Dropout(0.3)(dense_1)\n",
    "    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n",
    "    dense_2_dp = Dropout(0.3)(dense_2)\n",
    "    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n",
    "    dense_3_dp = Dropout(0.3)(dense_3)\n",
    "\n",
    "    \n",
    "    # Create input layer for wide component\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "\n",
    "    \n",
    "    # Concatenate the outputs of deep and wide components and feed the \n",
    "    # concatenated vector into the finall fully connected layer\n",
    "    fc_input = Concatenate()([dense_3_dp, wide_input])\n",
    "    model_output = Dense(1)(fc_input)\n",
    "    \n",
    "    model = Model(inputs=input_list,\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions to get the values of different types of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -continuous_columns: column names of continuous features\n",
    "    \n",
    "return: \n",
    "    a numpy array where each row contains the values of continuous features in the corresponding row of the\n",
    "    input dataframe\n",
    "'''\n",
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross product transformation of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -comb_p: number of elements in each combination (e.g., there are two elements in the combination {fried chicken, chicken and \n",
    "    waffle}, and three elements in the combination {fried chicken, chicken and waffle, chicken fried rice})\n",
    "    -topk: number of mostly frequent combinations to retrieve\n",
    "    -output_freq: whether to return the frequencies of retrieved combinations\n",
    "    \n",
    "return:\n",
    "    1. output_freq = True: a list X where each element is a tuple containing a combinantion tuple and corresponding frequency, and the \n",
    "        elements are stored in the descending order of their frequencies\n",
    "    2. output_freq = False: a list X where each element is a tuple containing a combinantion tuple, and the elements are stored in \n",
    "    the descending order of their frequencies\n",
    "'''\n",
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    # get all combinations with comb_p\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    # [('Lounges', 'Dance Clubs'), ('Lounges', 'Bars'), ('Lounges', 'Nightlife'), ('Dance Clubs', 'Bars'), ('Dance Clubs', 'Nightlife'), ('Bars', 'Nightlife')]\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    # ('Lounges', 'Dance Clubs')\n",
    "    # list of tuples that each index refer to one combination\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -selected_categories_to_idx: a dictionary mapping item categories to corrresponding integral indices\n",
    "    -top_combinations: a list containing retrieved mostly frequent combinantions of item categories\n",
    "    \n",
    "return:\n",
    "    a numpy array where each row contains the categorical features' binary encodings and cross product\n",
    "    transformations for the corresponding row of the input dataframe\n",
    "'''\n",
    "\n",
    "def get_wide_features(df, selected_categories_to_idx, top_combinations):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    print('category_binary_features shape:',category_binary_features.shape)\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    print('category_cross_features shape:',category_corss_transform_features.shape)\n",
    "    out = np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n",
    "    print('wide features shape:',out.shape)\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, validation and test rating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")\n",
    "te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load content feautures tables of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"data/user.csv\")\n",
    "item_df = pd.read_csv(\"data/business.csv\")\n",
    "\n",
    "# Rename some columns of dfs and convert the indices of dfs into string type for easier reference in later stage \n",
    "user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate each row in the rating tables with corresponding user's and item's content features through merging the rating tables and content features tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original row indices of each rating table\n",
    "tr_df[\"index\"] = tr_df.index\n",
    "val_df[\"index\"]  = val_df.index\n",
    "te_df[\"index\"] = te_df.index\n",
    "\n",
    "tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns containing conitnuous features\n",
    "continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n",
    "                      \"user_review_count\", \"user_useful\", \"user_funny\",\n",
    "                      \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
    "                      \"item_review_count\", \"item_stars\"]\n",
    "\n",
    "# Get values of continous features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "\n",
    "# Standardize each feature by removing the mean of the training samples and scaling to unit variance.\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for more details.\n",
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare deep categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcify column names of deep categorical features\n",
    "item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "\n",
    "# An array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n",
    "item_deep_vocab_lens = []\n",
    "\n",
    "for col_name in item_deep_columns:\n",
    "    # Get all unique values of this deep categorical feature\n",
    "    tmp = item_df[col_name].unique()\n",
    "    \n",
    "    # Create a dictionary mapping each unique value to a unique integral index\n",
    "    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "    \n",
    "    # Get the number of unique values of this deep categorical features\n",
    "    item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "    \n",
    "    # Create a new column where each entry stores the integral index of this deep categorical feature's value in the same row\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x])\n",
    "\n",
    "\n",
    "# Create a dictionary mapping each business id to corresponding values of deep categorical features\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_categorical_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "\n",
    "# Creat numpy arrays storing corresponding deep categorical features' values of train/validation/test sets using the above mapping\n",
    "tr_deep_categorical_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "val_deep_categorical_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "te_deep_categorical_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare wide features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare binary encoding for each selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categories of all items \n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "\n",
    "# Sort all unique values of the item categories by their frequencies in descending order\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top 500 most frequent categories\n",
    "selected_categories = [t[0] for t in category_sorted[:500]]\n",
    "\n",
    "# Create a dictionary mapping each secleted category to a unique integral index\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "\n",
    "# Map all categories unseen in the item df to index 0\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "\n",
    "# Create a dictionary mapping each integral index to corresponding category\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare cross product transformation for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent categories combinantions using the utility function defined previously and store them in the folloing list\n",
    "top_combinations = []\n",
    "\n",
    "# Get top 50 most frequent two-categories combinantions in the train set\n",
    "\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n",
    "\n",
    "# Get top 30 most frequent three-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
    "\n",
    "# Get top 20 most frequent four-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
    "\n",
    "# Convert each combinantion in the list to a set data structure\n",
    "top_combinations = [set(t) for t in top_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_binary_features shape: (100000, 501)\n",
      "category_cross_features shape: (100000, 100)\n",
      "wide features shape: (100000, 601)\n",
      "category_binary_features shape: (10000, 501)\n",
      "category_cross_features shape: (10000, 100)\n",
      "wide features shape: (10000, 601)\n",
      "category_binary_features shape: (10000, 501)\n",
      "category_cross_features shape: (10000, 100)\n",
      "wide features shape: (10000, 601)\n"
     ]
    }
   ],
   "source": [
    "# Get values of wide features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_wide_features = get_wide_features(tr_df, selected_categories_to_idx, top_combinations)\n",
    "val_wide_features = get_wide_features(val_df, selected_categories_to_idx, top_combinations)\n",
    "te_wide_features = get_wide_features(te_df, selected_categories_to_idx, top_combinations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the input list for each of the train/validation/test sets through aggregating all continuous, deep categorical and wide features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_features = [tr_continuous_features,categorical_features_0,categorical_features_1,categorical_features_2,tr_wide_features]\n",
    "tr_features = []\n",
    "tr_features.append(tr_continuous_features.tolist())\n",
    "tr_features += [tr_deep_categorical_features[:,i].tolist() for i in range(tr_deep_categorical_features.shape[1])]\n",
    "tr_features.append(tr_wide_features.tolist())\n",
    "\n",
    "\n",
    "\n",
    "val_features = []\n",
    "val_features.append(val_continuous_features.tolist())\n",
    "val_features += [val_deep_categorical_features[:,i].tolist() for i in range(val_deep_categorical_features.shape[1])]\n",
    "val_features.append(val_wide_features.tolist())\n",
    "\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features.tolist())\n",
    "te_features += [te_deep_categorical_features[:,i].tolist() for i in range(te_deep_categorical_features.shape[1])]\n",
    "te_features.append(te_wide_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the WDL model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdl_model = build_wdl_model(\n",
    "        len(tr_continuous_features[0]),\n",
    "        item_deep_vocab_lens,   # num of category classes\n",
    "        len(tr_wide_features[0]), \n",
    "        embed_size=100)\n",
    "#print(len(tr_continuous_features[0]))\n",
    "#print(item_deep_vocab_lens)\n",
    "#print(len(tr_wide_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using Adagrad optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\coding\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 7s 75us/step - loss: 1.3641\n"
     ]
    }
   ],
   "source": [
    "wdl_model.compile(optimizer='adagrad', loss='mse')\n",
    "\n",
    "history = wdl_model.fit(\n",
    "        tr_features, \n",
    "        tr_ratings, \n",
    "        epochs=1, verbose=1, callbacks=[ModelCheckpoint('model.h5')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on train and validation sets using RMSE¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  1.039801672098216\n",
      "VALID RMSE:  1.0475014041928454\n"
     ]
    }
   ],
   "source": [
    "y_pred = wdl_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = wdl_model.predict(val_features)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
